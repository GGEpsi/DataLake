{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîç Atelier Data Lake - Exploration des Donn√©es\n",
        "\n",
        "Ce notebook vous guide dans l'exploration des donn√©es de notre Data Lake.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration et imports\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from src.spark_session import get_spark_session\n",
        "from config.settings import BRONZE_PATH, SILVER_PATH, GOLD_PATH, SOURCES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cr√©ation de la session Spark\n",
        "spark = get_spark_session(\"DataLake_Exploration\")\n",
        "print(\"‚úÖ Session Spark cr√©√©e!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Exploration des Sources de Donn√©es\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lecture des clients (CSV)\n",
        "clients_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(str(SOURCES[\"csv\"] / \"clients.csv\"))\n",
        "\n",
        "print(f\"Nombre de clients: {clients_df.count()}\")\n",
        "print(\"\\nSch√©ma:\")\n",
        "clients_df.printSchema()\n",
        "print(\"\\nAper√ßu:\")\n",
        "clients_df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lecture des produits (CSV)\n",
        "produits_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(str(SOURCES[\"csv\"] / \"produits.csv\"))\n",
        "\n",
        "print(f\"Nombre de produits: {produits_df.count()}\")\n",
        "produits_df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lecture des commandes (JSON)\n",
        "from pyspark.sql.functions import explode, col\n",
        "\n",
        "commandes_raw = spark.read.option(\"multiline\", True).json(str(SOURCES[\"json\"] / \"commandes.json\"))\n",
        "commandes_df = commandes_raw.select(explode(\"commandes\").alias(\"c\")).select(\"c.*\")\n",
        "\n",
        "print(f\"Nombre de commandes: {commandes_df.count()}\")\n",
        "commandes_df.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Statistiques Descriptives\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistiques sur les produits\n",
        "produits_df.describe([\"prix\", \"stock\"]).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution par cat√©gorie\n",
        "produits_df.groupBy(\"categorie\").count().orderBy(\"count\", ascending=False).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution des clients par ville\n",
        "clients_df.groupBy(\"ville\").count().orderBy(\"count\", ascending=False).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution des commandes par statut\n",
        "commandes_df.groupBy(\"statut\").count().orderBy(\"count\", ascending=False).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Qualit√© des Donn√©es\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# V√©rifier les valeurs nulles\n",
        "from pyspark.sql.functions import count, when, isnan, isnull\n",
        "\n",
        "def check_nulls(df, df_name):\n",
        "    print(f\"\\n=== Valeurs nulles dans {df_name} ===\")\n",
        "    null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
        "    null_counts.show()\n",
        "\n",
        "check_nulls(clients_df, \"Clients\")\n",
        "check_nulls(produits_df, \"Produits\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# V√©rifier les doublons\n",
        "print(f\"Clients uniques: {clients_df.select('client_id').distinct().count()} / {clients_df.count()}\")\n",
        "print(f\"Produits uniques: {produits_df.select('produit_id').distinct().count()} / {produits_df.count()}\")\n",
        "print(f\"Commandes uniques: {commandes_df.select('commande_id').distinct().count()} / {commandes_df.count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fermeture de la session\n",
        "spark.stop()\n",
        "print(\"Session Spark ferm√©e.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
